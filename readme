when we first start working with gpus, we quickly become concerned with
    how do i get some code to run on it?
    how is this code interacting with the gpu?

suppose we want to execute some code path on the gpu
    we refer to this action as an invocation
    this invocation may not necessarily be independent
    this invocation may be mapped to multiple lanes
    it may not guarantee forward progress at every time step

we must be aware of possible constraints before making assumptions of its behavior

let's consider the monolithic gpu

┌───────────┐                  
│Scalar Unit│ for control flow, pointer/scalar arithmetic, shared operations, etc.
└───────────┘                  
┌───────────┐
│Scalar Reg.│ register file for the scalar unit, 12KB
└───────────┘
we can consider the scalar unit and scalar unit register file as a shared resource within a compute unit

┌───────────┐
│   SIMDx   │ vector processor with 16 Lanes
└───────────┘
┌───────────┐
│Vector Reg.│ vector register file 64KB :: 256 64x4 byte registers
└───────────┘
a single simd processor in a compute unit has 16 lanes, 
-- each of these lanes executes a synchronized vector operation (say a multiply-accumulate) on some 32 bit value
we also refer to these as work-items, thus, 1 simd unit in a compute unit has 16 work-items 
we call a collection of 64 work items a wavefront
we consider the wavefront as the atomic scheduling unit
each SIMD unit may buffer instructions for 10 wavefronts -- more on this later

┌───────────┐
│ L1 Cache  │ L1 vector data cache, ~16KB
└───────────┘
┌─────────────────────────────────────────────────────┐
│                 Local Data Share                    │
└─────────────────────────────────────────────────────┘
Shared local data store, 32 banks with conflict resolution, totaling 64KB

┌───────────────────────────────────────────────────────────────────┐
│                            Scheduler                              │
└───────────────────────────────────────────────────────────────────┘
buffers up to 40 wavefronts == 2560 work-items

-- we can imagine our gpu dispatching 1 wavefront to 1 compute unit which is pinned on 1 simd unit--

it is NOT the case that we map a wavefronts 64 work items to 4 simd units,
a wave front is only mapped to ONE simd unit
thus, it takes a simd unit 4 cycles to complete exeuction of a wavefront


thus, we have the Compute Unit
┌───────────────────────────────────────────────────────────────────┐
│                           Scheduler                               │
└───────────────────────────────────────────────────────────────────┘
┌───────────┐ ┌─────────────────────────────────────────────────────┐
│ L1 Cache  │ │                 Local Data Share                    │
└───────────┘ └─────────────────────────────────────────────────────┘
┌───────────┐ ┌───────────┐ ┌───────────┐ ┌───────────┐ ┌───────────┐
│Scalar Unit│ │   SIMD0   │ │   SIMD1   │ │   SIMD2   │ │   SIMD3   │
└───────────┘ └───────────┘ └───────────┘ └───────────┘ └───────────┘
┌───────────┐ ┌───────────┐ ┌───────────┐ ┌───────────┐ ┌───────────┐
│Scalar Reg.│ │Vector Reg.│ │Vector Reg.│ │Vector Reg.│ │Vector Reg.│
└───────────┘ └───────────┘ └───────────┘ └───────────┘ └───────────┘
*we will assume userspace, and using existing API*
